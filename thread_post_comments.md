# LinkedIn Post and Comments

## Post

Here's what nobody's saying about AI governance in regulated industries: The regulatory requirements and the technology are fundamentally incompatible. Not 'hard to align' - actually incompatible. Current regulatory frameworks assume you can trace decisions back through invertible processes. AI systems - especially LLMs - compress information in fundamentally non-invertible ways. Regulators ask: 'Show me the evidence chain for this decision.' The honest answer is often: 'The architecture compressed that information away. That's how the system works.' This isn't a compliance problem organizations can solve with better documentation. It's a fundamental mismatch between what regulators expect and what's information-theoretically possible. We need regulatory frameworks that understand compression dynamics, not frameworks that pretend AI is just fancy deterministic software.

## Comments

1. Exactly â€” and this is the key distinction. The failure isnâ€™t that AI decisions are non-invertible. Itâ€™s that governance is still being applied after compression, instead of being embedded before and during execution. Evidence doesnâ€™t need full reversibility of the model â€” it needs deterministic capture of intent, authority, constraints, and state at decision time. When those are treated as first-class runtime artifacts, the â€œevidence chainâ€ survives even when the model itself canâ€™t be unpacked. Thatâ€™s the layer most frameworks are missing.â€¦ more
2. What strikes me is that other regulated domains have already navigated similar territory. Financial regulators accept statistical models where individual predictions canâ€™t be fully explained in traditional terms. theyâ€™ve developed frameworks around validation, stress testing, and monitoring outputs rather than requiring perfect transparency into every calculation. The question isnâ€™t whether AI can meet frameworks designed for rule-based systems (it canâ€™t), but whether regulators can adapt validation approaches that accept inherent black box while still ensuring accountability. That means shifting focus from explain every step to prove your system behaves consistently and safely across scenarios we care about. The gap is real, but Iâ€™m not sure itâ€™s unbridgeable.â€¦ more
3. Non-invertibility breaks decision reconstruction, not representation evidence. You can't recover what the model compressed away, but you can preserve what external AI systems presented at the moment reliance occurred. Governance needs to shift from model-internal explainability to time-bound evidentiary capture at the representation layer, rather than treating LLM's as auditable deterministic software.
4. Jennifer F. K. appreciate you naming this as an information-theoretic problem instead of pretending â€œmore documentationâ€ will fix it. Youâ€™re right: if governance means reconstructing the internal state of a large model, most current regulatory expectations and LLM architectures simply donâ€™t match. The nuance Iâ€™d add from the legal side is that courts and regulators donâ€™t actually need full invertibility of the model; they need a dependable ğ—²ğ˜ƒğ—¶ğ—±ğ—²ğ—»ğ—°ğ—² ğ—°ğ—µğ—®ğ—¶ğ—» ğ—®ğ˜ ğ˜ğ—µğ—² ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—¯ğ—¼ğ˜‚ğ—»ğ—±ğ—®ğ—¿ğ˜†. In other words: who allowed this ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—³ğ—¶ğ—° ğ—®ğ—°ğ˜ğ—¶ğ—¼ğ—» to run, under which ğ—®ğ˜‚ğ˜ğ—µğ—¼ğ—¿ğ—¶ğ˜ğ˜†, and what ğ—¿ğ˜‚ğ—¹ğ—² ğ—½ğ—¼ğ˜€ğ˜ğ˜‚ğ—¿ğ—² applied at that moment? In our world weâ€™ve had to solve that by shifting the target: we donâ€™t try to make the modelâ€™s internal reasoning auditable, we require a ğ—½ğ—¿ğ—²-ğ—²ğ˜…ğ—²ğ—°ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ğ—®ğ˜‚ğ˜ğ—µğ—¼ğ—¿ğ—¶ğ˜ğ˜† ğ—´ğ—®ğ˜ğ—² in front of high-risk actions (file / send / approve / move) and emit ğ˜€ğ—²ğ—®ğ—¹ğ—²ğ—± ğ—±ğ—²ğ—°ğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ—®ğ—¿ğ˜ğ—¶ğ—³ğ—®ğ—°ğ˜ğ˜€ for every approve / refuse / supervised override. Itâ€™s a different kind of traceability: not â€œğ˜„ğ—µğ˜† this token,â€ but â€œğ˜„ğ—µğ—¼ owned this yes/no, and here is the evidence.â€â€¦ more
5. This goes one layer deeper. Compression doesnâ€™t just remove traceability; it embeds normative assumptions directly into the architecture. Thatâ€™s not a documentation gap; itâ€™s an institutional blind spot. Regulators ask for evidence chains, while the system operates through learned priors never designed to be externally legible.
6. Youâ€™re right about the mismatch â€” but itâ€™s only unsolvable if we insist on explaining the model instead of governing the system. Regulators donâ€™t actually need invertible reasoning; they need decision legitimacy. Thatâ€™s achievable by enforcing authority, scope, and responsibility before execution, with verifiable receipts. We already do this in payments, aviation, and safety systems. AI needs pre-decision governance, not post-hoc introspection.â€¦ more
7. There is a useful simplification here. Complex systems do not fail because they are complex. They fail because responsibility is left undefined at the point of action. This is an old lesson. Turing showed us that computation can be rigorously bounded even when behaviour is not easily interpretable. The discipline was never to explain every internal step, but to define the conditions under which a system may proceed â€” and when it must stop. Good governance follows the same rule. It does not demand full internal transparency. It demands explicit authority, declared limits, and accountable control at the moment decisions are made. Once that boundary is fixed, complexity becomes manageable. Without it, no amount of analysis ever is.
8. Most AI governance focuses on outputs, while failure modes originate internally. Weâ€™re prototyping systems where unsafe internal states are not penalized â€” theyâ€™re structurally unreachable. That distinction changes what â€œgovernanceâ€ even means.
9. Completely agree â€” If decision processes are fundamentally non-invertible, how should governance be designed to remain enforceable at runtime, not just explainable after the fact?
10. Jennifer F. K. I hear your frustration and support the call for ongoing development of governance structures. But I do think regulators are not inherently more clear about the conditions they face anymore than firms are. It is a learning curve the like of which have not been see before for all concerned. Yes, we need clarity but we need collective engagement as well.
11. The data structure can also be deined. When it was developed it was built to solve a need without the regulations but it has grown and now the legal wrapper is trying to cover it. Define the data structure. That is all.
12. The expectation within financial services is always explainability and auditability. The industry is scrambling to create a standard understanding and terminology across institutions and regulators
13. Itâ€™s a proxy problem , hard to fix, it can be done , we have filed Provisional patents that gets right to your point , hopefully we can make it available soon , lots of claims out there right now saying they have solved this also . Time will tell who has the full stack of licensed patches for the worlds broken AI models
14. Guess who's writing the legislation and how much they "understand" how AI (or at least any software) works
15. Governance by design meaning choosing Admissibility over Accuracy/Sophistication is great path to pursue. This is a critical consideration, thanks for bringing it up.
16. Needs a control surface is all. Layer 0
17. please check out policyedgeai we are actively working on a solution to resolve this pain point.
18. ğŸ”Rajeev Chakraborty
19. Respectfully, I disagree. I've read the vast majority of frameworks (NIST, OECD, EI AI Act, 42001 to name a few). I've then mapped them to each other and to SDLC, and cross referenced DP and Cybersec. Armed with this I've helped organisations embed AIG into their SDLC. What's needed are people with indepth knowledge and experience who have a foot in both camps.
20. Youâ€™re correct on the fundamental incompatibility, and itâ€™s worse for AI agents. When agents interpret context as authorization for financial transactions, regulators donâ€™t just need the decision trace. They need proof of who authorized the agent to interpret that context as intent. Iâ€™ve been mapping this: probabilistic compression meets deterministic accountability requirements. Thatâ€™s the unsolvable gap.

## Replies

1. Ricky Jones I'm not entirely sure about that. I get what you're saying but there's a saying; complex companies have complex problems, and create complex solutions. The hardest part is, in fact, sticking with it long enough to find the useful simplification...
2. Elinor MacKinnon I agree â€” simplification often only appears after spending real time inside complexity. The distinction I was trying to draw is about *when* that process applies. During exploration, complexity is something you work through. At the moment of action, especially under risk, complexity has to be bounded in advance. In engineering terms: you can explore indefinitely in design, but once a system is live, safety comes from explicit authority, clear limits, and known stop conditions â€” not from continued analysis. Thatâ€™s the layer Iâ€™m pointing at when I talk about governance.
